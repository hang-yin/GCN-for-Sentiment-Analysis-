{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819de6ca-d430-4f8e-b0a6-ef9a9d863da0",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b24a0-52fb-400b-9c55-759400a6740e",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab547d55-0695-40f4-ad7c-4c405d68a2fb",
   "metadata": {},
   "source": [
    "To explore dependency parsing for sentences so that word level graphs can be constructed for the problem of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e65a81-4093-4e69-8b9f-e0ed0dbc56e6",
   "metadata": {},
   "source": [
    "1. Explore how dependency parsing works\n",
    "2. Tokenize sentences and give them to the dependency parser\n",
    "3. Obtain the relationships and check them\n",
    "4. Convert the dependency graphs from the parser into the graphs that can be used for GCN\n",
    "5. Explore GCN and try to build graphs from the parsed dependency graphs\n",
    "6. Perform graph level classification\n",
    "7. Check performance\n",
    "8. Refine the various layers and hyperparameters in the process\n",
    "9. Check final performance\n",
    "10. Do interpreation manually on some selected examples\n",
    "11. Implement edge masking to get the minimum subgraph\n",
    "12. Performe automatic interpretation of the graphs to determine the most important terms/relationships for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992a7d6-b508-46b6-90e8-6c6706745cb3",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db8681-dabf-4d46-9a9e-f7bfc39beec4",
   "metadata": {},
   "source": [
    "* For dependency parsing, we will try to use the tools from the stanford nlp group.\n",
    "* The *[CoreNLP](https://github.com/stanfordnlp/CoreNLP)* library is important as it contains many of the tools needed to performe dependency parsing\n",
    "* The *[stanza](https://github.com/stanfordnlp/stanza)* library offers a wrapped version of CoreNLP that works directly with Python\n",
    "* Visualization of the graphs can be done using *[networkx](https://github.com/networkx/networkx)*\n",
    "* For GCN and word-level graphs we will primarily look at the ideas in this *[tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html)*\n",
    "* We will try to use the *[spectral](https://github.com/danielegrattarola/spektral/)* library to implement the graphs convolution and attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dca2a-d5ee-4d5a-a8bc-0a08312f5d66",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc056b-4209-4386-83f3-341763d644c6",
   "metadata": {},
   "source": [
    "The dataset is still IMDB review but we may expand later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed5318-0c20-4844-abc3-0557fdd87e50",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0b039-0e9a-49ba-9ae2-0cb73f941207",
   "metadata": {},
   "source": [
    "Fill in later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b995cc3-558e-442b-b019-c7118b9f68c6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be4892-b7c4-4e33-8bc5-87016be64a8a",
   "metadata": {},
   "source": [
    "Fill in at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614bc035-6d55-4c30-8e08-b74200f23079",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181f48c-7f2e-4f99-b114-9b2e558305ba",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b809df-9776-4c7b-a943-f1d8922bee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f4a952-8c39-4d69-ad06-c4f7b2f56b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d46c2e-722e-4e93-8f02-929d11abb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f9cc0-2104-41d0-9876-c09c88005ef2",
   "metadata": {},
   "source": [
    "## Part 1: Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155e9e9-0808-42d8-9e07-e7a339578f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25783253-4991-4f64-ad3c-cfe053630f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838ce42-7b58-45ec-aa1a-74f63b6f9c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761b022-43c7-44f5-8c95-89dadb82ba55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd77ca-7012-48c5-b671-5a2b3ca20355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a94f2a-7e8d-4282-a63e-7f52de8df506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
