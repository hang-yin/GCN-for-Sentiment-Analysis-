{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bc5f4233-f3dc-4b83-9c8e-fedafeb73041",
      "metadata": {
        "id": "bc5f4233-f3dc-4b83-9c8e-fedafeb73041"
      },
      "source": [
        "# Abstract for the Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793b87ce-4aba-43e4-a19a-3ef6910df878",
      "metadata": {
        "id": "793b87ce-4aba-43e4-a19a-3ef6910df878"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96b3d88-211a-46a2-8029-f91ba4023be6",
      "metadata": {
        "id": "f96b3d88-211a-46a2-8029-f91ba4023be6"
      },
      "source": [
        "The task is to test which model architecture performs best at imdb sentiment classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d4617e-384e-444d-97cf-663141d35e98",
      "metadata": {
        "id": "20d4617e-384e-444d-97cf-663141d35e98"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39586fc7-bdb3-41f5-af12-75a5047dec29",
      "metadata": {
        "id": "39586fc7-bdb3-41f5-af12-75a5047dec29"
      },
      "source": [
        "The models utilize BERT embeddings and various combinations of lstm , cnn , and graph techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2beddffa-3a84-4c52-9f2d-7cda2ed81c95",
      "metadata": {
        "id": "2beddffa-3a84-4c52-9f2d-7cda2ed81c95"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3238a08d-07a3-4bb0-881a-9b53737cc1ec",
      "metadata": {
        "id": "3238a08d-07a3-4bb0-881a-9b53737cc1ec"
      },
      "source": [
        "The experiments are in progress. We are looking for accuracy, failure points, and interpretability. The goal is to test for full data, limited data, and limited label scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "017cfee9-d0b0-40b5-813b-a18e39af04d9",
      "metadata": {
        "id": "017cfee9-d0b0-40b5-813b-a18e39af04d9"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f386a-76fe-4af7-911e-ad7d29cb3fac",
      "metadata": {
        "id": "cc7f386a-76fe-4af7-911e-ad7d29cb3fac"
      },
      "source": [
        "The main dataset is the IMDB movie reviews and whatever was used to produce the BERT embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b7b98f-2c66-4438-8a47-2c4f8e4ddbba",
      "metadata": {
        "id": "c1b7b98f-2c66-4438-8a47-2c4f8e4ddbba"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a602346-56ca-4a16-88ce-2dc585b155f2",
      "metadata": {
        "id": "2a602346-56ca-4a16-88ce-2dc585b155f2"
      },
      "source": [
        "## Import statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ooc42YZHqHn8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooc42YZHqHn8",
        "outputId": "7c09ae5a-86ca-42c9-db1c-3ace9a70931b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet tensorflow-text==2.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZcAQzY_YP5NJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcAQzY_YP5NJ",
        "outputId": "4f6eca5e-78ed-44f7-c585-c3f805cc63d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▊                             | 10 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 120 kB 5.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet neural_structured_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88bd8afe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88bd8afe",
        "outputId": "9feab477-a2de-442d-c085-e013cad5e4e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 102 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 112 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 122 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 129 kB 4.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet spektral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "33cb8783-f4e7-4f35-8ea8-f6bc1e30584a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33cb8783-f4e7-4f35-8ea8-f6bc1e30584a",
        "outputId": "ddd4890c-53d7-44b2-80c6-044cce2425be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version:  2.8.2\n",
            "Eager mode:  True\n",
            "Hub version:  0.12.0\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import neural_structured_learning as nsl\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import pandas as pd\n",
        "\n",
        "from spektral.data import Graph\n",
        "from spektral.data import Dataset\n",
        "from spektral.transforms import GCNFilter\n",
        "import pandas as pd\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.layers import GCNConv\n",
        "from spektral.models.gcn import GCN\n",
        "from spektral.transforms import LayerPreprocess\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Resets notebook state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\n",
        "    \"GPU is\",\n",
        "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "777323d4",
      "metadata": {
        "id": "777323d4"
      },
      "source": [
        "## GCN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "554fd299",
      "metadata": {
        "id": "554fd299"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f4b35b14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4b35b14",
        "outputId": "a911fd2a-e0da-465e-ed2a-dcda817e750a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "# dataset = Citation(data, normalize_x=True, transforms=[LayerPreprocess(GCNConv)])\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(pp_train_data, pp_train_labels), (pp_test_data, pp_test_labels) = (\n",
        "    imdb.load_data(num_words=10000))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eGI7DEOxIlQM",
      "metadata": {
        "id": "eGI7DEOxIlQM"
      },
      "outputs": [],
      "source": [
        "# This block limits how much is loaded to keep debugging short\n",
        "# don't run on the final experiments\n",
        "validation_fraction = 0.2\n",
        "limit = 25000\n",
        "\n",
        "pp_validation_data = pp_train_data[0: int(limit * validation_fraction)]\n",
        "pp_validation_labels = pp_train_labels[0: int(limit * validation_fraction)]\n",
        "pp_train_data = pp_train_data[int(limit * validation_fraction): limit]\n",
        "pp_train_labels = pp_train_labels[int(limit * validation_fraction): limit]\n",
        "pp_test_data = pp_test_data[0:limit]\n",
        "pp_test_labels = pp_test_labels[0:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "YqKhzLNj1xQB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqKhzLNj1xQB",
        "outputId": "656d1378-d0f4-4cb7-c07b-d247ab52ab68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training entries: 20000, labels: 20000\n"
          ]
        }
      ],
      "source": [
        "print('Training entries: {}, labels: {}'.format(\n",
        "    len(pp_train_data), len(pp_train_labels)))\n",
        "training_samples_count = len(pp_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "77334eed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77334eed",
        "outputId": "3ca62016-c937-487c-e821-f55dff63f76f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation entries: 5000, labels: 5000\n"
          ]
        }
      ],
      "source": [
        "print('Validation entries: {}, labels: {}'.format(\n",
        "    len(pp_validation_data), len(pp_validation_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a426e070",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a426e070",
        "outputId": "3204c939-a7ba-4eec-ecc2-84248d950a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test entries: 25000, labels: 25000\n"
          ]
        }
      ],
      "source": [
        "print('Test entries: {}, labels: {}'.format(\n",
        "    len(pp_test_data), len(pp_test_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "TBZF3Dy5115r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBZF3Dy5115r",
        "outputId": "a3bca6bc-4d9b-4a4c-e316-c355e39dab28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 13, 104, 14, 2856, 77, 6, 542, 20, 5, 49, 7, 94, 531, 26, 608, 21, 11, 226, 45, 6, 503, 20, 172, 44, 4, 114, 531, 26, 608, 21, 12, 47, 450, 1512, 5, 2, 15, 152, 179, 763, 56, 116, 9, 669, 608, 207, 110, 433, 7, 14, 99, 10, 10, 4, 454, 732, 4, 651, 19, 1880, 1953, 11, 4, 2092, 231, 646, 21, 484, 1744, 2980, 143, 49, 392, 2, 11, 550, 1270, 646, 40, 4, 9331, 69, 115, 110, 6439, 159, 12, 214, 1076, 128, 39, 50, 21, 24, 76, 10, 10, 48, 335, 2146, 851, 14, 79, 160, 31, 99, 5, 361, 14, 31, 17, 6, 2272, 56, 401, 1398, 364, 5, 12, 238, 157, 18, 25]\n"
          ]
        }
      ],
      "source": [
        "print(pp_train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dmvSSg9O13tE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmvSSg9O13tE",
        "outputId": "fdb1aafd-ecff-43d6-8e51-b4fc6d389848"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(124, 171)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pp_train_data[0]), len(pp_train_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "YbLPfq6y15wT",
      "metadata": {
        "id": "YbLPfq6y15wT"
      },
      "outputs": [],
      "source": [
        "def sampleFunction(inputFeature):\n",
        "    \"\"\"\n",
        "    This is a description of the function\n",
        "    \n",
        "    Args:\n",
        "        inputFeature - (np.ndarray) This is what the feature is\n",
        "    Returns:\n",
        "        result - (int) This is what is returned\n",
        "    \"\"\"\n",
        "\n",
        "    result = 55\n",
        "    return result\n",
        "\n",
        "def buildReverseWordIndex(dataset):\n",
        "    \"\"\"\n",
        "    Convert the index back to words with proper accounting for \n",
        "    the special characters reserved at the beginning of the dictionary\n",
        "\n",
        "    Args: \n",
        "        dataset - (keras.dataset) The dataset to use\n",
        "    Returns:\n",
        "        buildReverseWordIndex - (dict) A dictionary mapping words to an integer index\n",
        "    \"\"\"\n",
        "    wordIndex = dataset.get_word_index()\n",
        "\n",
        "    # The first indices are reserved\n",
        "    wordIndex = {k: (v + 3) for k, v in wordIndex.items()}\n",
        "    wordIndex['<PAD>'] = 0\n",
        "    wordIndex['<START>'] = 1\n",
        "    wordIndex['<UNK>'] = 2  # unknown\n",
        "    wordIndex['<UNUSED>'] = 3\n",
        "    return dict((value, key) for (key, value) in wordIndex.items())\n",
        "\n",
        "def decodeReview(text, reverseWordIndex):\n",
        "    \"\"\"\n",
        "    Uses build_reverse_word_index to decode original data format into text\n",
        "    \n",
        "    Args:\n",
        "        text - (np.ndarray) The text to decode\n",
        "        reverseWordIndex - (dict) The reverse word index to use\n",
        "    Returns:\n",
        "        decodedReview - (string) The decoded review\n",
        "    \"\"\"\n",
        "    return ' '.join([reverseWordIndex.get(i, '?') for i in text])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "SPdl56b8Ra_J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "SPdl56b8Ra_J",
        "outputId": "7ad4411c-9374-46fa-e5fe-0dae88abd17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> i think this could've been a decent movie and some of its parts are ok but in whole it's a b movie same about the plot parts are ok but it has several holes and <UNK> that doesn't quite add up acting is mostly ok i've seen worse of this too br br the beginning sets the level with cars driving in the desert making cool but totally unnecessary jumps through some small <UNK> in slow motion cool like the drivers had never seen sand before it gets slightly better from there but not much br br if you're gonna rent this get another one too and use this one as a warm up keep expectations low and it might work for you\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reverseWordIndex = buildReverseWordIndex(imdb)\n",
        "decodeReview(pp_train_data[0], reverseWordIndex)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1223e000",
      "metadata": {
        "id": "1223e000"
      },
      "source": [
        "### Generate BERT Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "73edd71d",
      "metadata": {
        "id": "73edd71d"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "722cb125",
      "metadata": {
        "id": "722cb125"
      },
      "outputs": [],
      "source": [
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "preprocessor = hub.KerasLayer(\n",
        "    'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "866efae2",
      "metadata": {
        "id": "866efae2"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = preprocessor(text_input)\n",
        "\n",
        "encoder = hub.KerasLayer(\n",
        "    'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2',\n",
        "    trainable=True)\n",
        "\n",
        "outputs = encoder(encoder_inputs)\n",
        "\n",
        "pooled_output = outputs['pooled_output'] # [batch_size, 128].\n",
        "# [batch_size, seq_length, 128].\n",
        "\n",
        "sequence_output = outputs['sequence_output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "82af6486",
      "metadata": {
        "id": "82af6486"
      },
      "outputs": [],
      "source": [
        "def int64Feature(value):\n",
        "    \"\"\"\n",
        "    Returns int64 tf.train.Feature.\n",
        "\n",
        "    Args:\n",
        "        value - (np.ndarray) array of ints\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "\n",
        "\n",
        "def bytesFeature(value):\n",
        "    \"\"\"\n",
        "    Returns bytes tf.train.Feature.\n",
        "\n",
        "    Args:\n",
        "        value - (string) string\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
        "\n",
        "\n",
        "def floatFeature(value):\n",
        "    \"\"\"\n",
        "    Returns float tf.train.Feature.\n",
        "\n",
        "    Args:\n",
        "        value - (np.ndarray) array of floats\n",
        "\n",
        "    \"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "000667cc",
      "metadata": {
        "id": "000667cc"
      },
      "outputs": [],
      "source": [
        "def createBertEmbeddingExample(wordVector, recordID, reverseWordIndex, encoder, preprocessor):\n",
        "    \"\"\"\n",
        "    Create tf.Example containing the sample's embedding and its ID.\n",
        "    \n",
        "    Args:\n",
        "        wordVector - (np.ndarray) the text to decode\n",
        "        recordId - (int) ID of the sample\n",
        "        reverseWordIndex - (dict) The reverse word index to use\n",
        "        encoder - (string) encoder name\n",
        "        preprocessor - (string) preprocessor name\n",
        "    Returns:\n",
        "        example - (tf.Example) tf.Example containing the sample's embedding and its ID\n",
        "    \"\"\"\n",
        "\n",
        "    text = decodeReview(wordVector, reverseWordIndex)\n",
        "\n",
        "    # Shape = [batch_size,].\n",
        "    sentenceEmbedding = encoder(preprocessor(tf.reshape(text, shape=[-1, ])))['pooled_output']\n",
        "    \n",
        "    # Flatten the sentence embedding back to 1-D.\n",
        "    sentenceEmbedding = tf.reshape(sentenceEmbedding, shape=[-1])\n",
        "    \n",
        "    features = {\n",
        "        'id': bytesFeature(str(recordID)),\n",
        "        'embedding': floatFeature(sentenceEmbedding.numpy())\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "\n",
        "def createBertEmbedding(wordVectors, outputPath, startingRecordId, reverseWordIndex, encoder, preprocessor):\n",
        "    \"\"\"\n",
        "    Create full set of BERT embeddings\n",
        "\n",
        "    Args:\n",
        "        wordVectors - (np.ndarray) all text to decode\n",
        "        outputPath - (string) path to output file\n",
        "        startingRecordId - (int) ID of the first sample\n",
        "        reverseWordIndex - (dict) The reverse word index to use\n",
        "        encoder - (string) encoder name\n",
        "        preprocessor - (string) preprocessor name\n",
        "    Returns:\n",
        "        recordID - (int) ID of the last sample\n",
        "    \"\"\"\n",
        "    recordID = int(startingRecordId)\n",
        "    with tf.io.TFRecordWriter(outputPath) as writer:\n",
        "        for word_vector in wordVectors:\n",
        "            example = createBertEmbeddingExample(word_vector, recordID, reverseWordIndex, encoder, preprocessor)\n",
        "            recordID = recordID + 1\n",
        "            writer.write(example.SerializeToString())\n",
        "    return recordID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "75b17e56",
      "metadata": {
        "id": "75b17e56"
      },
      "outputs": [],
      "source": [
        "!mkdir ./imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2a6ed47e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2a6ed47e",
        "outputId": "a8dc0804-a9e7-4158-c7ac-289e4cd1a0fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./imdb/bertEmeddings.tfr'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate full BERT embeddings\n",
        "\n",
        "bertEmbeddingsPath = './imdb/bertEmeddings.tfr'\n",
        "createBertEmbedding(pp_train_data, bertEmbeddingsPath, 0, reverseWordIndex, encoder, preprocessor)\n",
        "\n",
        "bertEmbeddingsPath_validation = './imdb/bertEmeddings_validation.tfr'\n",
        "createBertEmbedding(pp_validation_data, bertEmbeddingsPath_validation, 0, reverseWordIndex, encoder, preprocessor)\n",
        "\n",
        "bertEmbeddingsPath_test = './imdb/bertEmeddings_test.tfr'\n",
        "createBertEmbedding(pp_test_data, bertEmbeddingsPath_test, 0, reverseWordIndex, encoder, preprocessor)\n",
        "bertEmbeddingsPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "95a7d605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a7d605",
        "outputId": "3542bd83-8cb7-42f2-fcb8-d975e133110c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186718 ./imdb/bertEmeddings.tfr\n"
          ]
        }
      ],
      "source": [
        "!wc -l ./imdb/bertEmeddings.tfr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9e1dc192",
      "metadata": {
        "id": "9e1dc192"
      },
      "outputs": [],
      "source": [
        "def createBertEmbeddingExample_tf(wordVector, reverseWordIndex, encoder, preprocessor):\n",
        "    \"\"\"\n",
        "    Create tf.Example containing the sample's embedding and its ID.\n",
        "    \n",
        "    Args:\n",
        "        wordVector - (np.ndarray) the text to decode\n",
        "        recordId - (int) ID of the sample\n",
        "        reverseWordIndex - (dict) The reverse word index to use\n",
        "        encoder - (string) encoder name\n",
        "        preprocessor - (string) preprocessor name\n",
        "    Returns:\n",
        "        example - (tf.Example) tf.Example containing the sample's embedding and its ID\n",
        "    \"\"\"\n",
        "\n",
        "    text = decodeReview(wordVector, reverseWordIndex)\n",
        "\n",
        "    # Shape = [batch_size,].\n",
        "    sentenceEmbedding = encoder(preprocessor(tf.reshape(text, shape=[-1, ])))['pooled_output']\n",
        "    \n",
        "    # Flatten the sentence embedding back to 1-D.\n",
        "    sentenceEmbedding = tf.reshape(sentenceEmbedding, shape=[-1])\n",
        "    \n",
        "    # features = {\n",
        "    #     'id': bytesFeature(str(recordID)),\n",
        "    #     'embedding': floatFeature(sentenceEmbedding.numpy())\n",
        "    # }\n",
        "    # return tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    return sentenceEmbedding\n",
        "\n",
        "def createBertEmbedding_tf(wordVectors, reverseWordIndex, encoder, preprocessor):\n",
        "    \"\"\"\n",
        "    Create full set of BERT embeddings\n",
        "\n",
        "    Args:\n",
        "        wordVectors - (np.ndarray) all text to decode\n",
        "        outputPath - (string) path to output file\n",
        "        startingRecordId - (int) ID of the first sample\n",
        "        reverseWordIndex - (dict) The reverse word index to use\n",
        "        encoder - (string) encoder name\n",
        "        preprocessor - (string) preprocessor name\n",
        "    Returns:\n",
        "        recordID - (int) ID of the last sample\n",
        "    \"\"\"\n",
        "    sentenceEmbeddings = []\n",
        "    for word_vector in wordVectors:\n",
        "        example = createBertEmbeddingExample_tf(word_vector, reverseWordIndex, encoder, preprocessor)\n",
        "        sentenceEmbeddings.append(example)\n",
        "    sentenceEmbeddings_np = np.array(sentenceEmbeddings)\n",
        "    return sentenceEmbeddings_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dac10e0f",
      "metadata": {
        "id": "dac10e0f"
      },
      "outputs": [],
      "source": [
        "bertEmbeddings_np = createBertEmbedding_tf(pp_train_data, reverseWordIndex, encoder, preprocessor)\n",
        "bertEmbeddings_np\n",
        "\n",
        "bertEmbeddings_va_np = createBertEmbedding_tf(pp_validation_data, reverseWordIndex, encoder, preprocessor)\n",
        "bertEmbeddings_te_np = createBertEmbedding_tf(pp_test_data, reverseWordIndex, encoder, preprocessor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "79dc973b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79dc973b",
        "outputId": "3bd6c152-5e2b-46c3-9773-3ec320bbce69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 128)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(bertEmbeddings_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8cc287c",
      "metadata": {
        "id": "a8cc287c"
      },
      "source": [
        "### Construct Graph Using BERT Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "47af7aec",
      "metadata": {
        "id": "47af7aec"
      },
      "outputs": [],
      "source": [
        "similarity_threshold = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "07f26461",
      "metadata": {
        "id": "07f26461"
      },
      "outputs": [],
      "source": [
        "\n",
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=similarity_threshold, lsh_splits=32, lsh_rounds=15, random_seed=42)\n",
        "nsl.tools.build_graph_from_config(['./imdb/bertEmeddings.tfr'],\n",
        "                                  './imdb/graph_99.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bd4eab25",
      "metadata": {
        "id": "bd4eab25"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=similarity_threshold, lsh_splits=32, lsh_rounds=15, random_seed=42)\n",
        "nsl.tools.build_graph_from_config(['./imdb/bertEmeddings_test.tfr'],\n",
        "                                  './imdb/graph_99_test.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "15da11a6",
      "metadata": {
        "id": "15da11a6"
      },
      "outputs": [],
      "source": [
        "graph_builder_config = nsl.configs.GraphBuilderConfig(\n",
        "    similarity_threshold=similarity_threshold, lsh_splits=32, lsh_rounds=15, random_seed=42)\n",
        "nsl.tools.build_graph_from_config(['./imdb/bertEmeddings_validation.tfr'],\n",
        "                                  './imdb/graph_99_validation.tsv',\n",
        "                                  graph_builder_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1cd97b07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cd97b07",
        "outputId": "dc20d16e-08df-4a3a-fb92-dea4fec06fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24066 ./imdb/graph_99.tsv\n"
          ]
        }
      ],
      "source": [
        "!wc -l ./imdb/graph_99.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c6a8db",
      "metadata": {
        "id": "a4c6a8db"
      },
      "source": [
        "### Convert NSL Graph to Spektral Graph\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d2cb15b6",
      "metadata": {
        "id": "d2cb15b6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./imdb/graph_99.tsv', sep=\"\\t\") \n",
        "imdb_graph_a_adjacency_matrix = df.values\n",
        "size = len(pp_train_data)\n",
        "imdb_graph_a = np.zeros((size, size))\n",
        "for row in imdb_graph_a_adjacency_matrix:\n",
        "  imdb_graph_a[int(row[0]),int(row[1])] = row[2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "72aea348",
      "metadata": {
        "id": "72aea348"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./imdb/graph_99_validation.tsv', sep=\"\\t\") \n",
        "imdb_graph_va_a_adjacency_matrix = df.values\n",
        "size = len(pp_validation_data)\n",
        "imdb_graph_va_a = np.zeros((size, size))\n",
        "for row in imdb_graph_va_a_adjacency_matrix:\n",
        "  imdb_graph_va_a[int(row[0]),int(row[1])] = row[2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0ab8ff8b",
      "metadata": {
        "id": "0ab8ff8b"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./imdb/graph_99_test.tsv', sep=\"\\t\") \n",
        "imdb_graph_te_a_adjacency_matrix = df.values\n",
        "size = len(pp_test_data)\n",
        "imdb_graph_te_a = np.zeros((size, size))\n",
        "for row in imdb_graph_te_a_adjacency_matrix:\n",
        "  imdb_graph_te_a[int(row[0]),int(row[1])] = row[2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b31b15f4",
      "metadata": {
        "id": "b31b15f4"
      },
      "outputs": [],
      "source": [
        "max_seq_length_slice = 256\n",
        "# pp_train_data_sliced = np.zeros((len(pp_train_data), max_seq_length_slice))\n",
        "# i = 0\n",
        "# while i < len(pp_train_data):\n",
        "#   if len(pp_train_data[i]) > max_seq_length_slice:\n",
        "#     pp_train_data_sliced[i] = pp_train_data[i][:max_seq_length_slice]\n",
        "#   else:\n",
        "#     pp_train_data_sliced[i] = np.append(np.array(pp_train_data[i]), np.zeros(max_seq_length_slice - len(pp_train_data[i])))\n",
        "#   i = i + 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6e6a83ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6a83ee",
        "outputId": "4e5b8641-1876-485c-abe4-51e0326f0890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 128\n",
            "(20000, 128)\n"
          ]
        }
      ],
      "source": [
        "print(len(bertEmbeddings_np),len(bertEmbeddings_np[0]))\n",
        "print(bertEmbeddings_np.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "11780416",
      "metadata": {
        "id": "11780416"
      },
      "outputs": [],
      "source": [
        "pp_train_labels_binary = np.zeros((len(pp_train_labels),2))\n",
        "j = 0\n",
        "for _ in pp_train_labels:\n",
        "  if _ == 1:\n",
        "    pp_train_labels_binary[j][1] = 1\n",
        "  else:\n",
        "    pp_train_labels_binary[j][0] = 1\n",
        "  j = j + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6247f322",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6247f322",
        "outputId": "2547b076-7dc6-43ca-e7f5-6e8142296b4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pp_validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d835b96b",
      "metadata": {
        "id": "d835b96b"
      },
      "outputs": [],
      "source": [
        "pp_validation_labels_binary = np.zeros((len(pp_validation_labels),2))\n",
        "j = 0\n",
        "for _ in pp_validation_labels:\n",
        "  if _ == 1:\n",
        "    pp_validation_labels_binary[j][1] = 1\n",
        "  else:\n",
        "    pp_validation_labels_binary[j][0] = 1\n",
        "  j = j + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c3983468",
      "metadata": {
        "id": "c3983468"
      },
      "outputs": [],
      "source": [
        "pp_test_labels_binary = np.zeros((len(pp_test_labels),2))\n",
        "j = 0\n",
        "for _ in pp_test_labels:\n",
        "  if _ == 1:\n",
        "    pp_test_labels_binary[j][1] = 1\n",
        "  else:\n",
        "    pp_test_labels_binary[j][0] = 1\n",
        "  j = j + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "d7552242",
      "metadata": {
        "id": "d7552242"
      },
      "outputs": [],
      "source": [
        "imdb_graph=Graph(a=imdb_graph_a, x=bertEmbeddings_np, y=pp_train_labels_binary)\n",
        "imdb_graph_va=Graph(a=imdb_graph_va_a, x=bertEmbeddings_va_np, y=pp_validation_labels_binary)\n",
        "imdb_graph_te=Graph(a=imdb_graph_te_a, x=bertEmbeddings_te_np, y=pp_test_labels_binary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "75c3d704",
      "metadata": {
        "id": "75c3d704"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset of five random graphs.\n",
        "    \"\"\"\n",
        "    def __init__(self, graph, **kwargs):\n",
        "        # self.nodes = nodes\n",
        "        # self.feats = feats\n",
        "        self.graph = graph\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def download(self):\n",
        "        # data = ...  # Download from somewhere\n",
        "        path = './imdb'\n",
        "        # Create the directory\n",
        "        \n",
        "        # os.mkdir(path)\n",
        "        # filename = os.path.join(path, 'imdb_graph')\n",
        "        # np.savez(filename, x=imdb_graph.x, a=imdb_graph.a, y=imdb_graph.y)\n",
        "\n",
        "    def read(self):\n",
        "        # We must return a list of Graph objects\n",
        "        output = []\n",
        "\n",
        "        # for i in range(5):\n",
        "        #     data = np.load(os.path.join(self.path, f'graph_{i}.npz'))\n",
        "        #     output.append(\n",
        "        #         Graph(x=data['x'], a=data['a'], y=data['y'])\n",
        "        #     )\n",
        "        output.append(self.graph)\n",
        "        return output           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f1b9fc",
      "metadata": {
        "id": "d9f1b9fc"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset(imdb_graph)\n",
        "dataset[0]\n",
        "dataset.apply(GCNFilter())\n",
        "\n",
        "dataset_va = MyDataset(imdb_graph_va)\n",
        "dataset_va.apply(GCNFilter())\n",
        "\n",
        "dataset_te = MyDataset(imdb_graph_te)\n",
        "dataset_te.apply(GCNFilter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909e32aa",
      "metadata": {
        "id": "909e32aa"
      },
      "outputs": [],
      "source": [
        "dataset[0].x.shape\n",
        "dataset_va[0].x.shape\n",
        "dataset_te[0].x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b97d2a",
      "metadata": {
        "id": "91b97d2a"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3360f521",
      "metadata": {
        "id": "3360f521"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "learning_rate = 1e-3\n",
        "seed = 0\n",
        "epochs = 100\n",
        "patience = 10\n",
        "\n",
        "tf.random.set_seed(seed=seed)  # make weight initialization reproducible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e96291c",
      "metadata": {
        "id": "2e96291c"
      },
      "outputs": [],
      "source": [
        "model = GCN(n_labels=imdb_graph.n_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465b907d",
      "metadata": {
        "id": "465b907d"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=Adam(learning_rate),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    weighted_metrics=[\"acc\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b0cf8b",
      "metadata": {
        "id": "19b0cf8b"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "loader_tr = SingleLoader(dataset)\n",
        "loader_va = SingleLoader(dataset_va)\n",
        "model.fit(\n",
        "    loader_tr.load(),\n",
        "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
        "    validation_data = loader_va.load(),\n",
        "    validation_steps = loader_va.steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    # callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3115f9ef",
      "metadata": {
        "id": "3115f9ef"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82781529",
      "metadata": {
        "id": "82781529"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "print(\"Evaluating model.\")\n",
        "loader_te = SingleLoader(dataset_te)\n",
        "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
        "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bc5f4233-f3dc-4b83-9c8e-fedafeb73041",
        "3b7e1cb4-9f77-4a43-8162-24cc7cd0808a",
        "869dbe34-05f0-4a8a-b4e8-e4efd504986a",
        "6ba369a2-7dfc-462a-a769-8a5d1bc73a4f",
        "1ed0968c-139e-49ef-ba57-0c9f0c979f72"
      ],
      "name": "GCN Section.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "73e43853552d6fc356b2c05e3ac4c7e6c4dba3aed2cbc26a744008cb0f5a4666"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
